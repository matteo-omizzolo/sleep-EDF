# Hierarchical Dirichlet Process Hidden Semi-Markov Model for Sleep Staging

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A Bayesian nonparametric approach to unsupervised sleep staging using the sticky HDP-HMM on PhysioNet Sleep-EDF data.

## Overview

This project implements and evaluates hierarchical Dirichlet process hidden Markov models (HDP-HMMs) with sticky self-transitions for unsupervised sleep stage segmentation. The work directly extends the theoretical framework of:

- **Teh et al. (2006)**: Hierarchical Dirichlet Processes for sharing mixture components across groups
- **Fox et al. (2011)**: Sticky HDP-HMM for realistic state persistence in time series

### Research Question

**In unsupervised sleep staging, does hierarchical sharing of latent states across subjects via a sticky HDP-HMM improve (i) predictive performance and (ii) parsimony/consistency of discovered statesâ€”relative to fitting independent nonparametric HMMs per subject?**

## Key Features

- **Hierarchical state sharing**: Global stick-breaking prior enables state reuse across subjects
- **Sticky self-transitions**: Parameter Îº encourages realistic dwell times (avoids micro-segmentation)
- **Rigorous evaluation**: Leave-one-subject-out (LOSO) cross-validation with predictive likelihood and label agreement metrics
- **Novel domain**: First application of sticky HDP-HMM to polysomnography sleep staging

## Dataset: Sleep-EDF Expanded

- **Source**: [PhysioNet Sleep-EDF Database Expanded](https://physionet.org/content/sleep-edfx/1.0.0/)
- **Content**: 197 whole-night polysomnography recordings with expert hypnogram labels
- **Signals**: EEG (Fpz-Cz, Pz-Oz), EOG, EMG at 100 Hz
- **Labels**: Sleep stages {W, N1, N2, N3, REM} in 30-second epochs

### Why This Dataset?

- **Groups**: Each subject (or night) represents a group in the HDP hierarchy
- **Sequences**: Time series of 30-second epochs naturally map to HMM observations
- **Shared structure**: Biological sleep stages recur across subjects â†’ ideal for hierarchical sharing
- **Labeled ground truth**: Enables unsupervised-to-supervised evaluation via Hungarian matching

## Models

### 1. Independent DP-HMM (iDP-HMM) â€” Baseline

Each subject m has its own infinite HMM with DP prior over transitions:
- No cross-subject sharing of states or parameters
- Baseline to quantify the value of hierarchical structure

### 2. Sticky HDP-HMM â€” Target Model

- **Global stick-breaking**: Î² ~ GEM(Î³) shared across all subjects
- **Subject-specific transitions**: Ï€_j^(m) ~ DP(Î±+Îº, (Î±Î² + ÎºÎ´_j)/(Î±+Îº))
- **Stickiness parameter**: Îº biases self-transitions to encourage realistic dwell times
- **Gaussian emissions**: y_t | s_t ~ N(Î¼_k, Î£_k) with NIW prior

### 3. Pooled iHMM (Optional Control)

Single infinite HMM fit to all subjects jointly (no group variation) to demonstrate over-merging when hierarchy is removed.

## Project Structure

```
sleep-EDF/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ setup.py                     # Package installation
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ LICENSE                      # MIT License
â”‚
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â”œâ”€â”€ default_config.yaml      # Default hyperparameters
â”‚   â””â”€â”€ experiment_configs/      # Specific experiment settings
â”‚
â”œâ”€â”€ data/                        # Data directory (not tracked)
â”‚   â”œâ”€â”€ raw/                     # Raw Sleep-EDF files (.edf, .txt)
â”‚   â””â”€â”€ processed/               # Preprocessed features (.npy, .pkl)
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/                    # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ download.py          # PhysioNet data downloader
â”‚   â”‚   â”œâ”€â”€ loader.py            # EDF file reader
â”‚   â”‚   â””â”€â”€ preprocessing.py     # Feature extraction (PSD, bandpowers)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                  # Model implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py              # Base HMM interface
â”‚   â”‚   â”œâ”€â”€ idp_hmm.py           # Independent DP-HMM per subject
â”‚   â”‚   â”œâ”€â”€ hdp_hmm_sticky.py    # Sticky HDP-HMM (main model)
â”‚   â”‚   â””â”€â”€ pooled_ihmm.py       # Single pooled infinite HMM
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/               # MCMC inference
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sampler.py           # Gibbs/beam sampler
â”‚   â”‚   â”œâ”€â”€ weak_limit.py        # Truncated stick-breaking approximation
â”‚   â”‚   â””â”€â”€ diagnostics.py       # Convergence checks (R-hat, ESS)
â”‚   â”‚
â”‚   â”œâ”€â”€ eval/                    # Evaluation metrics and plotting
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py           # ARI, NMI, F1, log-likelihood
â”‚   â”‚   â”œâ”€â”€ hungarian.py         # State-to-label alignment
â”‚   â”‚   â””â”€â”€ plots.py             # All visualization functions
â”‚   â”‚
â”‚   â””â”€â”€ utils/                   # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # Configuration loading
â”‚       â””â”€â”€ logger.py            # Logging setup
â”‚
â”œâ”€â”€ scripts/                     # Executable scripts
â”‚   â”œâ”€â”€ download_data.py         # Download Sleep-EDF from PhysioNet
â”‚   â”œâ”€â”€ preprocess_all.py        # Preprocess all subjects
â”‚   â”œâ”€â”€ run_idp_hmm.py           # Run independent baseline
â”‚   â”œâ”€â”€ run_hdp_hmm_sticky.py    # Run sticky HDP-HMM
â”‚   â””â”€â”€ run_loso_cv.py           # Full LOSO cross-validation
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_comparison.ipynb
â”‚   â””â”€â”€ 04_results_visualization.ipynb
â”‚
â”œâ”€â”€ results/                     # Output directory (not tracked)
â”‚   â”œâ”€â”€ figures/                 # Generated plots
â”‚   â”œâ”€â”€ tables/                  # Summary tables (CSV/LaTeX)
â”‚   â””â”€â”€ models/                  # Saved posterior samples (.pkl)
â”‚
â””â”€â”€ tests/                       # Unit tests
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_data.py
    â”œâ”€â”€ test_models.py
    â””â”€â”€ test_inference.py
```

## Installation

### Prerequisites

- Python 3.8 or higher
- Virtual environment (recommended)

### Setup

```bash
# Clone the repository
git clone https://github.com/matteo-omizzolo/sleep-EDF.git
cd sleep-EDF

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Install package in editable mode
pip install -e .
```

## Usage

### 1. Download Data

```bash
python scripts/download_data.py --output data/raw --n-subjects 30
```

### 2. Preprocess Features

```bash
python scripts/preprocess_all.py \
    --input data/raw \
    --output data/processed \
    --features psd bandpower
```

Features extracted per 30-second epoch:
- Welch power spectral density (PSD)
- Log bandpowers: Î´ (0.5-4 Hz), Î¸ (4-8 Hz), Î± (8-12 Hz), Ïƒ (12-16 Hz), Î² (16-30 Hz)
- EOG variance, EMG variance (optional)

### 3. Run Models

#### Independent DP-HMM (baseline)

```bash
python scripts/run_idp_hmm.py \
    --data data/processed \
    --config configs/default_config.yaml \
    --output results/idp_hmm
```

#### Sticky HDP-HMM (target)

```bash
python scripts/run_hdp_hmm_sticky.py \
    --data data/processed \
    --config configs/default_config.yaml \
    --output results/hdp_hmm_sticky
```

#### Full LOSO Cross-Validation

```bash
python scripts/run_loso_cv.py \
    --data data/processed \
    --models idp_hmm hdp_hmm_sticky \
    --n-folds 30 \
    --output results/loso_cv
```

### 4. Analyze Results

Launch Jupyter notebooks:

```bash
jupyter notebook notebooks/03_model_comparison.ipynb
```

Or generate all figures programmatically:

```bash
python scripts/generate_figures.py --input results/loso_cv --output results/figures
```

## Experimental Design

### Cross-Validation Strategy

- **Leave-One-Subject-Out (LOSO)**: Fit on M-1 subjects, evaluate on held-out subject
- **Splits**: 20-30 subjects, 1 night per subject
- **Metrics**:
  - Predictive log-likelihood (test sequence)
  - Adjusted Rand Index (ARI)
  - Normalized Mutual Information (NMI)
  - Macro-F1 score (post Hungarian alignment)

### Hyperparameters & Priors

```yaml
# Default configuration (configs/default_config.yaml)
concentration:
  gamma: Gamma(1.0, 1.0)     # Global DP concentration
  alpha: Gamma(1.0, 1.0)     # Group-level DP concentration
  kappa: Gamma(5.0, 1.0)     # Sticky self-transition bias

emissions:
  prior: NIW                  # Normal-Inverse-Wishart
  mu_0: [0, ..., 0]          # Prior mean (feature dim)
  kappa_0: 0.01              # Prior precision scaling
  psi: I                     # Prior scale matrix
  nu: feature_dim + 2        # Prior degrees of freedom

inference:
  method: weak_limit          # or 'beam_sampler'
  K_max: 50                  # Truncation level
  n_iter: 5000               # Total MCMC iterations
  burn_in: 2000              # Burn-in iterations
  thin: 5                    # Thinning interval
  n_chains: 3                # Parallel chains for diagnostics
```

### MCMC Diagnostics

- **Convergence**: Gelman-Rubin R-hat < 1.1 for key parameters
- **Mixing**: Effective sample size (ESS) > 100
- **Trace plots**: Visual inspection of Î±, Î³, Îº, K
- **Label switching**: Addressed via sticky prior and post-processing

## Key Results (Expected)

### Figures & Tables for 20-Minute Talk

All figures are automatically generated and saved to `results/figures/`:

1. **`fig1_posterior_num_states.pdf`**: Posterior over K (global states vs per-subject fragmentation)
2. **`fig2_state_sharing_heatmap.pdf`**: Who uses which state (subjects Ã— global states)
3. **`fig3_dwell_times.pdf`**: Sticky mattersâ€”realistic segment durations
4. **`fig4_test_loglik_loso.pdf`**: Generalization to new subjects (boxplot)
5. **`fig5_ari_nmi_comparison.pdf`**: Biological plausibility (label agreement)
6. **`fig6_states_vs_subjects.pdf`**: E[K] growth with M (data scaling)
7. **`fig7_stick_breaking_weights.pdf`**: Global Î² (posterior mean Â± 95% CI)
8. **`fig8_hypnogram_examples.pdf`**: Representative reconstructions (2-3 subjects)
9. **`fig9_ablation_kappa.pdf`**: Effect of stickiness Îº on ARI/log-likelihood

**Table 1** (`tables/summary_table.tex`):

| Model            | E[K] | Median Dwell (s) | Test Log-Lik | ARI   | NMI   | Macro-F1 |
|------------------|------|------------------|--------------|-------|-------|----------|
| iDP-HMM          | ...  | ...              | ...          | ...   | ...   | ...      |
| HDP-HMM (sticky) | ...  | ...              | ...          | ...   | ...   | ...      |
| Pooled iHMM      | ...  | ...              | ...          | ...   | ...   | ...      |

## Alignment with Original Papers

### Teh et al. (2006): Hierarchical Dirichlet Processes
- **Core contribution**: Chinese restaurant franchise for sharing mixture components across groups
- **Our application**: Subjects = groups; sleep stages = shared components

### Fox et al. (2011): Sticky HDP-HMM
- **Core contribution**: Îº parameter biases self-transitions â†’ realistic dwell times
- **Our application**: Corrects over-segmentation in sleep time series

### Novel contribution of this work
- **Different domain**: Sleep staging (polysomnography) vs text topics or speaker diarization
- **Cross-subject generalization**: LOSO evaluation stresses utility of hierarchical sharing under distribution shift

## Reproducibility

All experiments are fully reproducible:
- Fixed random seeds in all scripts
- Configuration files track all hyperparameters
- MCMC diagnostics saved alongside results
- Python environment pinned via `requirements.txt`

To reproduce results from the paper/talk:

```bash
bash scripts/reproduce_all.sh
```

This script:
1. Downloads data
2. Preprocesses features
3. Runs all models with LOSO CV
4. Generates all figures and tables
5. Outputs to `results/paper/`

## References

### Core Methods
- Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006). *Hierarchical Dirichlet processes*. JASA.
- Fox, E. B., Sudderth, E. B., Jordan, M. I., & Willsky, A. S. (2011). *A sticky HDP-HMM with application to speaker diarization*. Annals of Applied Statistics.
- Van Gael, J., Saatci, Y., Teh, Y. W., & Ghahramani, Z. (2008). *Beam sampling for the infinite hidden Markov model*. ICML.

### Dataset
- Kemp, B., Zwinderman, A. H., Tuk, B., Kamphuisen, H. A., & Oberye, J. J. (2000). *Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the EEG*. IEEE-BME.
- Goldberger et al. (2000). *PhysioBank, PhysioToolkit, and PhysioNet*. Circulation.

### Related Work
- Stephens, M. (2000). *Dealing with label switching in mixture models*. JRSS-B.
- Johnson, M. J. & Willsky, A. S. (2013). *Bayesian nonparametric hidden semi-Markov models*. JMLR.

## License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## Citation

If you use this code in your research, please cite:

```bibtex
@misc{omizzolo2025hdphsmm_sleep,
  author = {Omizzolo, Matteo},
  title = {Hierarchical Dirichlet Process Hidden Semi-Markov Model for Sleep Staging},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/matteo-omizzolo/sleep-EDF}
}
```

## Contact

Matteo Omizzolo - [GitHub](https://github.com/matteo-omizzolo)

---

**Status**: ðŸš§ Work in progress â€” initial implementation phase
